{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6943251,"sourceType":"datasetVersion","datasetId":3987410}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\n\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:13:36.676946Z","iopub.execute_input":"2023-11-29T08:13:36.677384Z","iopub.status.idle":"2023-11-29T08:13:51.321853Z","shell.execute_reply.started":"2023-11-29T08:13:36.677350Z","shell.execute_reply":"2023-11-29T08:13:51.320348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(n_features):\n    '''\n        Create an MLP model.\n    '''\n    model = Sequential([\n        Dense(units = 128, input_shape = (n_features,), activation = 'relu'),\n        # Dropout(rate = 0.2),\n        Dense(units = 32, activation = 'relu'),\n        # Dropout(rate = 0.2),\n        Dense(units = 16, activation = 'relu'),\n        # Dropout(rate = 0.2),\n        Dense(units = 1, activation = 'sigmoid')\n    ])\n    \n    # Compile model.\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    \n    return model\n\ndef create_train_test_sets(train_df, test_df):\n    '''\n    '''\n    \n    # Drop domain name, family and Label column to create X_train data.\n    X_train = np.array(train_df.drop([\"Domain Name\", \"Family\", \"Label\"], axis = 1))\n\n    # Create a y np array with the labels.\n    y_train = np.array(train_df[\"Label\"])\n\n    # Drop domain name, family and Label column to create X_test data.\n    X_test = np.array(test_df.drop([\"Domain Name\", \"Family\", \"Label\"], axis = 1))\n\n    # Create a y np array with the labels.\n    y_test = np.array(test_df[\"Label\"])\n    \n    # Normalize datasets.\n    X_train, X_test = MinMax_normalization(X_train, X_test)\n    \n    return X_train, y_train, X_test, y_test\n\ndef MinMax_normalization(X_train, X_test):\n    '''\n        Normalize data using MinMax Normalization\n        \n            Input: \n                Train, validation and test set\n                \n            Return: \n                Scaled train, validation and test set\n    '''\n    \n    # Create a scaler based on train dataset.\n    scaler_obj = MinMaxScaler()\n    X_train_scaled = scaler_obj.fit_transform(X_train)\n    \n    # Transform validation and test sety based on the training scaler.\n    X_test_scaled = scaler_obj.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled\n\ndef k_fold_training_results_line_plot(train_results_dict):\n\n    # Create a figure with a 3x3 grid of subplots\n    fig, axs = plt.subplots(3, 2, figsize = (25, 10))  # You can adjust the figsize as needed\n\n    # Flatten the axs array for easier iteration\n    axs = axs.flatten()\n\n    # Create subplots with two lines in each subplot\n    for i, ax in enumerate(axs):\n        ax.plot(train_results_dict[str(i)][0], label = 'training loss')\n        ax.plot(train_results_dict[str(i)][1], label = 'Validation loss')\n        ax.set_title(f'Fold {i + 1}')\n        ax.legend()\n\n    # Adjust the layout and spacing\n    plt.tight_layout()\n    \n    plt.savefig('non_federated_mlp_loss_plot_kfold.png')\n\n    # Show the plots\n    plt.show()\n\n# def plot_training_results(train_results_dict):\n#     '''\n#         Create a loss plot after mlp training.\n#     '''\n    \n#     # Create a figure with a 3x3 grid of subplots\n#     plt.subplots(figsize = (15, 10))  # You can adjust the figsize as needed\n\n\n#     # Create subplots with two lines in each subplot\n#     plt.plot(train_results_dict['0'][0], label = 'training loss')\n#     plt.plot(train_results_dict['0'][1], label = 'Validation loss')\n#     plt.legend()\n\n#     # Adjust the layout and spacing\n#     plt.tight_layout()\n\n#     plt.savefig('new_mlp_training_loss_plot_100_epochs_32_batch.png')\n\n#     # Show the plots\n#     plt.show()\n\n# def create_train_validation_test_sets(train_df, test_df):\n#     '''\n#     '''\n    \n#     # Drop domain name, family and Label column to create X_train data.\n#     X_train = np.array(train_df.drop([\"Domain Name\", \"Family\", \"Label\"], axis = 1))\n\n#     # Create a y np array with the labels.\n#     y_train = np.array(train_df[\"Label\"])\n    \n#     # Split into train and validation set. \n#     X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n\n#     # Drop domain name, family and Label column to create X_test data.\n#     X_test = np.array(test_df.drop([\"Domain Name\", \"Family\", \"Label\"], axis = 1))\n\n#     # Create a y np array with the labels.\n#     y_test = np.array(test_df[\"Label\"])\n    \n#     # Normalize datasets.\n#     X_train, X_validation, X_test = MinMax_normalization(X_train, X_validation, X_test)\n    \n#     return X_train, y_train, X_validation, y_validation, X_test, y_test","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:23:53.198331Z","iopub.execute_input":"2023-11-29T08:23:53.199087Z","iopub.status.idle":"2023-11-29T08:23:53.214223Z","shell.execute_reply.started":"2023-11-29T08:23:53.199055Z","shell.execute_reply":"2023-11-29T08:23:53.212559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train on cpu only\n# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n# Fix path\nfinal_train_path = \"/kaggle/input/non-federated-train-test-df/final_train_df.csv\"\nfinal_test_path = \"/kaggle/input/non-federated-train-test-df/final_test_df.csv\"\n\n# Load final train and test dataframes.\ntrain_df = pd.read_csv(final_train_path, header = 0)\ntest_df = pd.read_csv(final_test_path, header = 0)\n\n# Create X and y arrays, for train and test sets.\nX_train, y_train, X_test, y_test = create_train_test_sets(train_df, test_df)\n\nno_folds = 6\nskf_obj = StratifiedKFold(n_splits = no_folds, shuffle = True, random_state = 42)\n\ntrain_results_dict = {}\nevaluation_results_dict = {}\nfor i, (train_index, test_index) in enumerate(skf_obj.split(X_train, y_train)):\n    \n    print('Fold: {}'.format(i))\n    \n    # Create current train and validation sets.\n    current_X_train = X_train[train_index]\n    current_y_train = y_train[train_index]\n    \n    X_validation = X_train[test_index]\n    y_validation = y_train[test_index]\n    \n    # Create MLP model.\n    n_features = current_X_train.shape[1]\n    mlp = create_model(n_features)\n    \n    # Train MLP on the dataset\n    history = mlp.fit(current_X_train, current_y_train, epochs = 100, batch_size = 32, validation_data = (X_validation, y_validation), verbose = 1)\n\n    # Save training results: training and validation loss.\n    train_results_dict[str(i)] = [history.history['loss'], history.history[\"val_loss\"]]\n    \n    # Get the predictions of the model.\n    y_pred = mlp.predict(X_test)\n    y_pred = y_pred.flatten()\n    y_pred = np.round(y_pred)\n    y_pred = y_pred.astype(int)\n    \n    # Save evaluation results: Accuracy, Precision, Recall, F1-score.\n    evaluation_results_dict[str(i)] = [accuracy_score(y_test, y_pred),\\\n                                       precision_score(y_test, y_pred),\\\n                                       recall_score(y_test, y_pred),\\\n                                       f1_score(y_test, y_pred)\\\n                                      ]\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2023-11-13T08:52:01.489184Z","iopub.execute_input":"2023-11-13T08:52:01.489668Z","iopub.status.idle":"2023-11-13T10:57:08.989057Z","shell.execute_reply.started":"2023-11-13T08:52:01.489629Z","shell.execute_reply":"2023-11-13T10:57:08.987256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_fold_training_results_line_plot(train_results_dict)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T10:57:12.777153Z","iopub.execute_input":"2023-11-13T10:57:12.777847Z","iopub.status.idle":"2023-11-13T10:57:15.414184Z","shell.execute_reply.started":"2023-11-13T10:57:12.777805Z","shell.execute_reply":"2023-11-13T10:57:15.413191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results_df = pd.DataFrame(evaluation_results_dict)\nevaluation_results_df = evaluation_results_df.transpose() \nevaluation_results_df.rename(columns = {0:'Accuracy', 1: 'Precision', 2: 'Recall', 3: 'F1-score'}, inplace = True)\nevaluation_results_df","metadata":{"execution":{"iopub.status.busy":"2023-11-13T11:07:25.981799Z","iopub.execute_input":"2023-11-13T11:07:25.982212Z","iopub.status.idle":"2023-11-13T11:07:25.999818Z","shell.execute_reply.started":"2023-11-13T11:07:25.982181Z","shell.execute_reply":"2023-11-13T11:07:25.998543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation_results_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T11:07:31.219925Z","iopub.execute_input":"2023-11-13T11:07:31.220780Z","iopub.status.idle":"2023-11-13T11:07:31.246920Z","shell.execute_reply.started":"2023-11-13T11:07:31.220742Z","shell.execute_reply":"2023-11-13T11:07:31.245791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explainable AI\n","metadata":{}},{"cell_type":"code","source":"import shap\n\n# Fix path\nfinal_train_path = \"/kaggle/input/non-federated-train-test-df/final_train_df.csv\"\nfinal_test_path = \"/kaggle/input/non-federated-train-test-df/final_test_df.csv\"\n\n# Load final train and test dataframes.\ntrain_df = pd.read_csv(final_train_path, header = 0)\ntest_df = pd.read_csv(final_test_path, header = 0)\n\n# Create X and y arrays, for train and test sets.\nX_train, y_train, X_test, y_test = create_train_test_sets(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:26:13.686082Z","iopub.execute_input":"2023-11-29T08:26:13.686416Z","iopub.status.idle":"2023-11-29T08:26:21.103776Z","shell.execute_reply.started":"2023-11-29T08:26:13.686389Z","shell.execute_reply":"2023-11-29T08:26:21.101993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create MLP model.\nn_features = X_train.shape[1]\nmlp = create_model(n_features)\n\n# Train MLP on the dataset\nhistory = mlp.fit(X_train, y_train, epochs = 100, batch_size = 32, verbose = 1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:27:34.177902Z","iopub.execute_input":"2023-11-29T08:27:34.178257Z","iopub.status.idle":"2023-11-29T08:53:48.408097Z","shell.execute_reply.started":"2023-11-29T08:27:34.178230Z","shell.execute_reply":"2023-11-29T08:53:48.406771Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the predictions of the model.\ny_pred = mlp.predict(X_test)\ny_pred = y_pred.flatten()\ny_pred = np.round(y_pred)\ny_pred = y_pred.astype(int)\n\nprint(accuracy_score(y_test, y_pred),precision_score(y_test, y_pred),recall_score(y_test, y_pred),f1_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:54:45.921299Z","iopub.execute_input":"2023-11-29T08:54:45.921689Z","iopub.status.idle":"2023-11-29T08:55:15.549811Z","shell.execute_reply.started":"2023-11-29T08:54:45.921659Z","shell.execute_reply":"2023-11-29T08:55:15.548706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.DeepExplainer(model = mlp, data = X_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:55:26.470272Z","iopub.execute_input":"2023-11-29T08:55:26.470686Z","iopub.status.idle":"2023-11-29T08:55:26.684377Z","shell.execute_reply.started":"2023-11-29T08:55:26.470655Z","shell.execute_reply":"2023-11-29T08:55:26.683221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute shapley values.\nshap_values = explainer.shap_values(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T08:56:41.247194Z","iopub.execute_input":"2023-11-29T08:56:41.247637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop([\"Domain Name\", \"Family\", \"Label\"], axis = 1).columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary_plot of a specific class\n\nclass_names = ['benign', 'malicious']\nshap.summary_plot(shap_values[1], X, plot_type = \"dot\", class_names = class_names, feature_names = df.drop('Action', axis = 1).columns)","metadata":{},"execution_count":null,"outputs":[]}]}